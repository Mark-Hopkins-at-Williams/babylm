Training 10 GPT2 models on the BabyLM data using all datasets provided excpet one (a unique dataset for each model).

A clone of the gpt2 experiment 


