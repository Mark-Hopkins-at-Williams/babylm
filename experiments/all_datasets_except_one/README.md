Training 10 GPT2 models on the BabyLM data using all datasets provided excpet one (a unique dataset for each model).

The base code is a clone of the gpt2 experiment 

Second run:
redo the experiments for leaving out 'aochildes', 'cbt', and 'gutenberg' datasets to see the variance in the results.


