Training 10 GPT2 models on the BabyLM data using all datasets provided excpet one (a unique dataset for each model).

The base code is a clone of the gpt2 experiment 


